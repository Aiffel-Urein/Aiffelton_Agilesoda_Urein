{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size = 32\n",
    "\n",
    "## Replay buffer\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n) #sample 메서드 : buffer 중에 n 개만 뽑음\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        # state 값, action 값, reward 값, 다음 state 값, done_mask 값\n",
    "        # done_mask : 종료 상태의 Value 값을 마스킹해줍니다\n",
    "\n",
    "        for transition in mini_batch: #우리가 뽑은 n 개의 미니배치들 하나씩에서\n",
    "            s, a, r, s_prime, done_mask = transition #\n",
    "\n",
    "            # 이로 보아 하나의 sample 에는 s,a,s_prime,done_mask 값이 담김을 확인할 수 있다\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a]) # 자료형이 list 임을 확인 가능, 여러개의 action 이 담겨있을 것이라 추측\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        return torch.tensor(s_lst, dtype=torch. float), torch.tensor(a_lst), torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch. float), torch.tensor(done_mask_lst)\n",
    "        # s_lst, s_prime_lst 는 타입을 바꿔주었다. 데이터가 int 였기 때문에\n",
    "        # 나머지는 tensor화\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 때 tensor 의 이해를 위해 기본 예제를 추가했다.\n",
    "\n",
    "### Tensor 의 기본 데이터 구조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1,2],[3,4]]\n",
    "x_data = torch.tensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "torch.int64\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "type(x_data)\n",
    "print(x_data.shape)\n",
    "print(x_data.dtype)\n",
    "print(x_data.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5911, 0.0660, 0.2966, 0.3930],\n",
       "        [0.1805, 0.9510, 0.5649, 0.0447],\n",
       "        [0.6955, 0.7548, 0.1732, 0.6378]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "다음은 Q net class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4,128) # 4가지 input 들어감\n",
    "        self.fc2 = nn.Linear(128,128)\n",
    "        self.fc3 = nn.Linear(128,2) #좌 인지 우 인지의 action state 값을 반환\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # 2개의 값을 반환\n",
    "        return x\n",
    "\n",
    "    def sample_action(self, obs, epsilon):\n",
    "        out = self.forward(obs) ### obs 에 무엇이 들어가는가? 카트의 위치,속도, 막대의 각도, 각속도 4개\n",
    "        coin = random.random() # 0~1 사이의 값을 랜덤하게 호출\n",
    "\n",
    "        if coin < epsilon:  # 동전던지기, 8% 확률로 랜덤하게 행동\n",
    "            return random.randint(0,1)\n",
    "        else:\n",
    "            return out.argmax().item() #Action state 값이 더 높은 index 값 호출\n",
    "\n",
    "## 학습 함수\n",
    "\n",
    "def train(q, q_target, memory, optimizer): \n",
    "    for i in range(10):\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size) #32개를 버퍼에서 뽑아 모아 놓은 s,a,r,s_prime,done_mask\n",
    "\n",
    "        q_out = q(s) # s 값으로 다음 각 action 값들의 value 값 반환\n",
    "        q_a = q_out.gather(1,a) #선택한 액션값들의 q(s,a) 반환\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1) # 다음 state의 각 q(s,a) 값 반환\n",
    "        target = r + gamma * max_q_prime * done_mask # 배열 맞춰주기, 쓰러진 경우는 제거\n",
    "        loss = F.smooth_l1_loss(q_a, target) # DQN 의 손실함수 계산 L1 유클리드\n",
    "\n",
    "        optimizer.zero_grad() # optimizer 의 모든 parameter 를 0으로 변환\n",
    "        loss.backward() # loss 에 대한 gradient 계산\n",
    "        optimizer.step() # 손실값을 바탕으로 Qnet 의 파라미터 업데이트\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### tensor.gather 에 대해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14],\n",
      "         [15, 16, 17]],\n",
      "\n",
      "        [[18, 19, 20],\n",
      "         [21, 22, 23]]])\n"
     ]
    }
   ],
   "source": [
    "# gather 메서드\n",
    "t = torch.tensor([i for i in range(4*2*3)]).reshape(4,2,3)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_A = torch.tensor([1,0,3])\n",
    "ind_A = ind_A.unsqueeze(1).unsqueeze(2)\n",
    "ind_A = ind_A.expand(ind_A.size(0), t.size(1), t.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [1, 1, 1]],\n",
       "\n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       "\n",
       "        [[3, 3, 3],\n",
       "         [3, 3, 3]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "res =t.gather(0,ind_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6,  7,  8],\n",
       "         [ 9, 10, 11]],\n",
       "\n",
       "        [[ 0,  1,  2],\n",
       "         [ 3,  4,  5]],\n",
       "\n",
       "        [[18, 19, 20],\n",
       "         [21, 22, 23]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    q = Qnet()\n",
    "    q_target = Qnet()\n",
    "    q_target.load_state_dict(q.state_dict()) # 현재 Qnet 의 파라미터를 q_target 에 load\n",
    "\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    print_interval = 20 #20회마다 출력\n",
    "    score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr = learning_rate) # loss 값을 바탕으로 업데이트할 비율 (q_target 말고 q 만 업데이트)\n",
    "\n",
    "\n",
    "    for n_epi in range(10000):\n",
    "        epsilon = max(0.01, 0.08 - 0.01 * (n_epi/200))\n",
    "        # exploration 비율 - 200번마다 1%씩 8%에서 1%로 감소\n",
    "\n",
    "        s = env.reset()\n",
    "        \n",
    "        done = False \n",
    "        while not done: # 게임이 끝날 때까지\n",
    "            a = q.sample_action(torch.from_numpy(s).float(), epsilon) #환경의 값이 numpy로 되어 있는데, 이걸 가져와서 \n",
    "            #신경망을 돌린 후 0,1 중 하나의 값을 선택\n",
    "\n",
    "            #s = cart 의 속도, 방향, Pole 의 각, 각속도\n",
    "            s_prime, r, done, info = env.step(a) # a를 선택했을때 s_prime, 종료여부, info\n",
    "            \n",
    "            done_mask = 0.0 if done else 1.0 # False 가 아직 안끝난 경우, true 가 끝난경우\n",
    "            # 안끝났을 때 done_mask = 0 , 끝나면 1\n",
    "\n",
    "            memory.put((s,a,r/100.0, s_prime, done_mask))\n",
    "\n",
    "            s = s_prime # 다음 s 값으로 이어서 진행\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            if memory.size() > 2000:\n",
    "                train(q, q_target, memory, optimizer) \n",
    "            # 일단 2000개의 epi 가 쌓이고 나서 학습 시작\n",
    "\n",
    "            if n_epi%print_interval==0 and n_epi != 0: # 200번마다 한번씩\n",
    "                q_target.load_state_dict(q.state_dict()) #q_target 지금걸로 업데이트\n",
    "                print(f\"n_episode :{n_epi}, score = {int(score/print_interval)}, n_buffer : {memory.size()}, eps : {epsilon*100}\")\n",
    "                score = 0.0\n",
    "\n",
    "                # 이때 score 는 200번 시도 평균임\n",
    "\n",
    "        env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUM 라이브러리에 대한 간단 메서드\n",
    "\n",
    "- make() : 만들고 싶은 환경 가져오기  \n",
    "- reset() : defalut 환경 구축  \n",
    "- step() : a 액션을 취했을 때에 다음 state로 넘어감  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02374508,  0.04786856,  0.04516721,  0.01752456], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " env = gym.make('CartPole-v1')\n",
    "\n",
    " s = env.reset()\n",
    "\n",
    " s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:163: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.24502157,  2.3899536 , -0.33426705, -3.6713626 ], dtype=float32),\n",
       " 0.0,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = env.step(1)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "환경이 끝났을 시, warn 경고문 반환"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a332f7e40325215be763eec0cae04fc83a17fd4fbdbafdb52ae1a5c2dd4e321"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
