{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe7344c",
   "metadata": {},
   "source": [
    "# 1. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b02b2de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -0.1 -0.5 10\n"
     ]
    }
   ],
   "source": [
    "from string import ascii_uppercase\n",
    "from draw_utils import *\n",
    "from pyglet.gl import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "\n",
    "# reward\n",
    "# reward\n",
    "move_reward = -0.1\n",
    "obs_reward = -0.5\n",
    "goal_reward = 10\n",
    "finish_reward = 20\n",
    "print('reward:' , move_reward, obs_reward, goal_reward)\n",
    "\n",
    "local_path = '/home/zlxlekta924/YC' #os.path.abspath(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        height : 그리드 높이\n",
    "        width : 그리드 너비 \n",
    "        inds : A ~ Q alphabet list\n",
    "        '''\n",
    "        # Load train data\n",
    "        self.files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_test.csv\")) #\"./data/factory_order_train.csv\"))\n",
    "        self.height = 10\n",
    "        self.width = 9\n",
    "        self.inds = list(ascii_uppercase)[:17]\n",
    "        \n",
    "        self.total_ac = 0\n",
    "\n",
    "    def set_box(self):\n",
    "        '''\n",
    "        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n",
    "        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n",
    "        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n",
    "        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n",
    "        '''\n",
    "        box_data = pd.read_csv(os.path.join(local_path, \"data/box.csv\"))\n",
    "\n",
    "        # 물건이 들어있을 수 있는 경우\n",
    "        for box in box_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 0\n",
    "\n",
    "        # 물건이 실제 들어있는 경우\n",
    "        order_item = list(set(self.inds) & set(self.items))\n",
    "        order_csv = box_data[box_data['item'].isin(order_item)]\n",
    "        \n",
    "        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = 200\n",
    "            # local target에 가야 할 위치 좌표 넣기\n",
    "            self.local_target.append(\n",
    "                [getattr(order_box, \"row\"),\n",
    "                 getattr(order_box, \"col\")]\n",
    "                )\n",
    "\n",
    "        #self.local_target.append([9,4]) \n",
    "        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n",
    "\n",
    "    def set_obstacle(self):\n",
    "        '''\n",
    "        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n",
    "        '''\n",
    "        obstacles_data = pd.read_csv(os.path.join(local_path, \"data/obstacles.csv\"))\n",
    "        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n",
    "\n",
    "    def reset(self, epi):\n",
    "        '''\n",
    "        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n",
    "\n",
    "        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n",
    "        :return: 초기셋팅 된 그리드\n",
    "        :rtype: numpy.ndarray\n",
    "        _____________________________________________________________________________________\n",
    "        items : 이번 에피소드에서 가져와야하는 아이템들\n",
    "        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n",
    "        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n",
    "        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n",
    "        curloc : 현재 위치\n",
    "        '''\n",
    "\n",
    "        # initial episode parameter setting\n",
    "        self.epi = epi\n",
    "        self.items = list(self.files.iloc[self.epi])[0]\n",
    "        self.cumulative_reward = 0\n",
    "        self.terminal_location = None\n",
    "        self.local_target = []\n",
    "        self.actions = []\n",
    "        self.item_loc = False ## 수정\n",
    "        \n",
    "        # initial grid setting\n",
    "        self.grid = np.ones((self.height, self.width), dtype=\"float16\")\n",
    "\n",
    "        # set information about the gridworld\n",
    "        self.set_box()\n",
    "        self.set_obstacle()\n",
    "\n",
    "        # start point를 grid에 표시\n",
    "        self.curloc = [9, 4]\n",
    "        self.grid[int(self.curloc[0])][int(self.curloc[1])] = 100\n",
    "        \n",
    "        self.done = False\n",
    "        \n",
    "        return self.grid\n",
    "\n",
    "    def apply_action(self, action, cur_x, cur_y):\n",
    "        '''\n",
    "        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n",
    "        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n",
    "        \n",
    "        :param x: 에이전트의 현재 x 좌표\n",
    "        :param y: 에이전트의 현재 y 좌표\n",
    "        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n",
    "        :rtype: int, int\n",
    "        '''\n",
    "        new_x = cur_x\n",
    "        new_y = cur_y\n",
    "        # up\n",
    "        if action == 0:\n",
    "            new_x = cur_x - 1\n",
    "        # down\n",
    "        elif action == 1:\n",
    "            new_x = cur_x + 1\n",
    "        # left\n",
    "        elif action == 2:\n",
    "            new_y = cur_y - 1\n",
    "        # right\n",
    "        else:\n",
    "            new_y = cur_y + 1\n",
    "\n",
    "        return int(new_x), int(new_y)\n",
    "\n",
    "\n",
    "    def get_reward(self, new_x, new_y, out_of_boundary):\n",
    "        '''\n",
    "        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n",
    "\n",
    "        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n",
    "        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n",
    "        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n",
    "        :return: action에 따른 리워드\n",
    "        :rtype: float\n",
    "        '''\n",
    "\n",
    "        # 바깥으로 나가는 경우\n",
    "        if any(out_of_boundary):\n",
    "            reward = obs_reward\n",
    "                       \n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 \n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                reward = obs_reward  \n",
    "\n",
    "            # 현재 목표에 도달한 경우\n",
    "            elif [new_x, new_y] in self.terminal_location:\n",
    "                if [new_x, new_y] == [9, 4]:\n",
    "                    reward = finish_reward\n",
    "                else:\n",
    "                    reward = goal_reward\n",
    "\n",
    "            # 그냥 움직이는 경우 \n",
    "            else:\n",
    "                reward = move_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        ''' \n",
    "        에이전트의 action에 따라 step을 진행한다.\n",
    "        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n",
    "        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n",
    "\n",
    "        :param action: 에이전트 행동\n",
    "        :return:\n",
    "            grid, 그리드\n",
    "            reward, 리워드\n",
    "            cumulative_reward, 누적 리워드\n",
    "            done, 종료 여부\n",
    "            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n",
    "\n",
    "        :rtype: numpy.ndarray, float, float, bool, bool/str\n",
    "\n",
    "        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n",
    "        '''\n",
    "\n",
    "        self.terminal_location = copy.deepcopy(self.local_target)\n",
    "        cur_x,cur_y = self.curloc\n",
    "        self.actions.append((cur_x, cur_y))\n",
    "\n",
    "        goal_ob_reward = False\n",
    "        \n",
    "        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n",
    "\n",
    "        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n",
    "\n",
    "        # 바깥으로 나가는 경우 종료\n",
    "        if any(out_of_boundary):\n",
    "            pass\n",
    "            #self.done = True\n",
    "            #goal_ob_reward = True\n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 종료\n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                pass\n",
    "                #self.done = True\n",
    "                #goal_ob_reward = True\n",
    "\n",
    "            # 현재 목표에 도달한 경우\n",
    "            elif [new_x, new_y] in self.terminal_location:\n",
    "\n",
    "                # end point 일 때\n",
    "                if [new_x, new_y] == [9,4]:\n",
    "                    \n",
    "                    self.done = True\n",
    "                    self.local_target.remove([new_x, new_y])\n",
    "                \n",
    "                # item 일때\n",
    "                else:\n",
    "                    self.local_target.remove([new_x, new_y])\n",
    "                    if not self.local_target:\n",
    "                        self.local_target.append([9,4])\n",
    "                        self.grid[9][4] = 200\n",
    "                \n",
    "                if self.item_loc: #저번에가 item 이었던 자리었으면\n",
    "                    self.grid[cur_x][cur_y] = 0\n",
    "                    self.grid[new_x][new_y] = 100\n",
    "                else:\n",
    "                    self.grid[cur_x][cur_y] = 1\n",
    "                    self.grid[new_x][new_y] = 100\n",
    "\n",
    "                goal_ob_reward = True\n",
    "                self.item_loc=True\n",
    "                \n",
    "                self.curloc = [new_x, new_y]\n",
    "            else:\n",
    "                # 그냥 움직이는 경우\n",
    "                if self.item_loc:\n",
    "                    self.grid[cur_x][cur_y] = 0\n",
    "                    self.grid[new_x][new_y] = 100\n",
    "                    self.item_loc = False\n",
    "\n",
    "                else:\n",
    "                    self.grid[cur_x][cur_y] = 1\n",
    "                    self.grid[new_x][new_y] = 100\n",
    "                    \n",
    "                self.curloc = [new_x,new_y]\n",
    "                \n",
    "        reward = self.get_reward(new_x, new_y, out_of_boundary)\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        if self.done == True:\n",
    "            if [new_x, new_y] == [9, 4]:\n",
    "                if self.terminal_location[0] == [9, 4]:\n",
    "                    pass\n",
    "\n",
    "                #  # 완료되면 GIFS 저장\n",
    "#                     self.total_ac += len(self.actions)\n",
    "#                     if len(self.actions) < 50:\n",
    "#                         pass\n",
    "                        \n",
    "#                         print(f'50번 안에 들어왔다! : {len(self.actions)}')\n",
    "#                         goal_ob_reward = 'finish'\n",
    "#                         height = 10\n",
    "#                         width = 9 \n",
    "#                         display = Display(visible=False, size=(width, height))\n",
    "#                         display.start()\n",
    "\n",
    "#                         start_point = (9, 4)\n",
    "#                         unit = 50\n",
    "#                         screen_height = height * unit\n",
    "#                         screen_width = width * unit\n",
    "#                         log_path = \"./logs\"\n",
    "#                         data_path = \"./data\"\n",
    "#                         render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "#                         for idx, new_pos in enumerate(self.actions):\n",
    "#                            render_cls.update_movement(new_pos, idx+1)\n",
    "\n",
    "#                         render_cls.save_gif(self.epi)\n",
    "#                         render_cls.viewer.close()\n",
    "#                         display.stop()\n",
    "#                     else:\n",
    "#                         pass\n",
    "# #                         print(f'{len(self.actions)}번 시행 후 완료')\n",
    "                        \n",
    "        \n",
    "        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward\n",
    "\n",
    "    def ac(self):\n",
    "        return self.total_ac\n",
    "    def canvas(self):\n",
    "        goal_ob_reward = 'finish'\n",
    "        height = 10\n",
    "        width = 9 \n",
    "        display = Display(visible=False, size=(width, height))\n",
    "        display.start()\n",
    "\n",
    "        start_point = (9, 4)\n",
    "        unit = 50\n",
    "        screen_height = height * unit\n",
    "        screen_width = width * unit\n",
    "        log_path = \"./logs\"\n",
    "        data_path = \"./data\"\n",
    "        render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "        for idx, new_pos in enumerate(self.actions):\n",
    "            render_cls.update_movement(new_pos, idx+1)\n",
    "\n",
    "        render_cls.save_gif(self.epi)\n",
    "        render_cls.viewer.close()\n",
    "        display.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3245a",
   "metadata": {},
   "source": [
    "## 1. Agent 구성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d6f061",
   "metadata": {},
   "source": [
    "## 1-1 PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "badc3db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "################################## PPO Policy ##################################\n",
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.is_terminals = []\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.actions[:]\n",
    "        del self.states[:]\n",
    "        del self.logprobs[:]\n",
    "        del self.rewards[:]\n",
    "        del self.is_terminals[:]\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,action_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "        \n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full((action_dim,), action_std_init * action_std_init)\n",
    "        # actor\n",
    "        if has_continuous_action_space :\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(9, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, 64),\n",
    "                            nn.Tanh(),\n",
    "                            nn.Linear(64, action_dim),\n",
    "                        )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(64, 16),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(16, action_dim),\n",
    "                            nn.Softmax(dim=-1)\n",
    "                        )\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "                            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Flatten(),\n",
    "                            nn.Linear(64, 16),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(16,1),\n",
    "                    )\n",
    "        \n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling ActorCritic::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        \n",
    "        return action.detach(), action_logprob.detach()\n",
    "    \n",
    "    def evaluate(self, state, action):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            \n",
    "            action_var = self.action_var.expand_as(action_mean)\n",
    "            cov_mat = torch.diag_embed(action_var)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "            \n",
    "            # For Single Action Environments.\n",
    "            if self.action_dim == 1:\n",
    "                action = action.reshape(-1, self.action_dim)\n",
    "        else:\n",
    "            state = torch.reshape(state, (-1, 1, 10, 9))\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        \n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "        \n",
    "        self.buffer = RolloutBuffer()\n",
    "\n",
    "        self.policy = ActorCritic(action_dim, has_continuous_action_space, action_std_init)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "                    ])\n",
    "\n",
    "        self.policy_old = ActorCritic(action_dim, has_continuous_action_space, action_std_init)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = new_action_std\n",
    "            self.policy.set_action_std(new_action_std)\n",
    "            self.policy_old.set_action_std(new_action_std)\n",
    "        else:\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "            print(\"WARNING : Calling PPO::set_action_std() on discrete action space policy\")\n",
    "            print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_std = self.action_std - action_std_decay_rate\n",
    "            self.action_std = round(self.action_std, 4)\n",
    "            if (self.action_std <= min_action_std):\n",
    "                self.action_std = min_action_std\n",
    "                print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "            else:\n",
    "                print(\"setting actor output action_std to : \", self.action_std)\n",
    "            self.set_action_std(self.action_std)\n",
    "\n",
    "        else:\n",
    "            print(\"WARNING : Calling PPO::decay_action_std() on discrete action space policy\")\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    def select_action(self, state):\n",
    "\n",
    "        if self.has_continuous_action_space:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "\n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.detach().cpu().numpy().flatten()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state)\n",
    "                action, action_logprob = self.policy_old.act(state)\n",
    "            \n",
    "            self.buffer.states.append(state)\n",
    "            self.buffer.actions.append(action)\n",
    "            self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "            return action.item()\n",
    "\n",
    "    def update(self):\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals)):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "            \n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        # convert list to tensor\n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            state_values = torch.squeeze(state_values)\n",
    "            \n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            advantages = rewards - state_values.detach()   \n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n",
    "            \n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        self.buffer.clear()\n",
    "    \n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "   \n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa0ed90b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "save checkpoint path : PPO_preTrainedPPO_Grid World_0_03.pth\n",
      "Started training at (GMT) :  2022-06-07 06:57:22\n",
      "============================================================================================\n",
      "총 시행 수 : 1226\n",
      "성공수 : 1226\n",
      "성공률 : 100.0%\n",
      "평균 행동 수 : 45.451876019575856\n",
      "time : 46.11190056800842\n",
      "============================================================================================\n",
      "Started training at (GMT) :  2022-06-07 06:57:22\n",
      "Finished training at (GMT) :  2022-06-07 06:58:08\n",
      "Total training time  :  0:00:46\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "################################### Training ###################################\n",
    "def train():\n",
    "    files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_test.csv\"))\n",
    "    \n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    ####### initialize environment hyperparameters ######\n",
    "    env_name = \"Grid World\"\n",
    "\n",
    "    has_continuous_action_space = False  # continuous action space; else discrete\n",
    "\n",
    "    max_ep_len = 200                   # max timesteps in one episode\n",
    "    max_training_timesteps = int(3e5)   # break training loop if timeteps > max_training_timesteps\n",
    "\n",
    "    print_freq = max_ep_len * 100        # print avg reward in the interval (in num timesteps)\n",
    "    log_freq = max_ep_len * 2           # log avg reward in the interval (in num timesteps)\n",
    "    save_model_freq = int(1e5)          # save model frequency (in num timesteps)\n",
    "\n",
    "    action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n",
    "    action_std_decay_rate = 0.05        # linearly decay action_std (action_std = action_std - action_std_decay_rate)\n",
    "    min_action_std = 0.1                # minimum action_std (stop decay after action_std <= min_action_std)\n",
    "    action_std_decay_freq = int(2.5e5)  # action_std decay frequency (in num timesteps)\n",
    "    #####################################################\n",
    "\n",
    "    ## Note : print/log frequencies should be > than max_ep_len\n",
    "\n",
    "    ################ PPO hyperparameters ################\n",
    "    update_timestep = max_ep_len * 4      # update policy every n timesteps\n",
    "    K_epochs = 10               # update policy for K epochs in one PPO update\n",
    "\n",
    "    eps_clip = 0.2          # clip parameter for PPO\n",
    "    gamma = 0.99            # discount factor\n",
    "\n",
    "    lr_actor = 0.00003       # learning rate for actor network 0.0003\n",
    "    lr_critic = 0.0003       # learning rate for critic network 0.001\n",
    "\n",
    "    random_seed = 0         # set random seed if required (0 = no random seed)\n",
    "    #####################################################\n",
    "\n",
    "    env = Simulator()\n",
    "    \n",
    "    action_dim = 4\n",
    "\n",
    "    ################### checkpointing ###################\n",
    "    run_num_pretrained = 0      #### change this to prevent overwriting weights in same env_name folder\n",
    "\n",
    "    directory = \"PPO_preTrained\"\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "\n",
    "\n",
    "    checkpoint_path = directory + \"PPO_{}_{}_{}3.pth\".format(env_name, random_seed, run_num_pretrained)\n",
    "    print(\"save checkpoint path : \" + checkpoint_path)\n",
    "\n",
    "    # initialize a PPO agent\n",
    "    ppo_agent = PPO(action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space, action_std)\n",
    "\n",
    "    # track total training time\n",
    "    start_time = datetime.now().replace(microsecond=0)\n",
    "    print(\"Started training at (GMT) : \", start_time)\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    # printing and logging variables\n",
    "    print_running_reward = 0\n",
    "    print_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "\n",
    "    ppo_agent.load(checkpoint_path) # 모델 불러오기\n",
    "    cnt = 0\n",
    "    to = 0\n",
    "    pe = 0\n",
    "    le = len(files)\n",
    "    # training loop\n",
    "    while time_step <= max_training_timesteps:\n",
    "        # 1 : 16, 2 : 136 ,3 : 680 , 4: 2380 , 5: 6188 , 6: 12261 , 7 : 39999(트레인)\n",
    "        start = time.time()  # 시작 시간 저장\n",
    "        for epi in range(le):\n",
    "            state = env.reset(epi)\n",
    "            current_ep_reward = 0\n",
    "            \n",
    "            for t in range(1, max_ep_len+1):\n",
    "                # state = torch.FloatTensor(state)\n",
    "                # state = state.reshape(1,-1)\n",
    "                state = torch.from_numpy(state).float()\n",
    "                state = torch.reshape(state, (-1, 1, 10, 9))\n",
    "\n",
    "                action = ppo_agent.select_action(state)\n",
    "\n",
    "                state, reward, cumul, done, goal_ob_reward = env.step(action)\n",
    "\n",
    "                # saving reward and is_terminals\n",
    "                ppo_agent.buffer.rewards.append(reward)\n",
    "                ppo_agent.buffer.is_terminals.append(done)\n",
    "\n",
    "                time_step +=1\n",
    "                current_ep_reward += reward\n",
    "\n",
    "                # update PPO agent\n",
    "                if time_step % update_timestep == 0:\n",
    "                    ppo_agent.update()\n",
    "\n",
    "                # printing average reward\n",
    "#                 if time_step % 20000 == 0:\n",
    "\n",
    "#                     # print average reward till last episode\n",
    "#                     print_avg_reward = print_running_reward / print_running_episodes\n",
    "#                     print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "#                     print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "#                     print_running_reward = 0\n",
    "#                     print_running_episodes = 0\n",
    "\n",
    "                # save model weights\n",
    "#                 if time_step % save_model_freq == 0:\n",
    "#                     ppo_agent.save(checkpoint_path)\n",
    "\n",
    "\n",
    "                # break; if the episode is over\n",
    "                if done:\n",
    "                    cnt+=1\n",
    "                    to +=t\n",
    "                    break\n",
    "                if t==max_ep_len:\n",
    "                    pe +=1\n",
    "                    print(epi, f'에서 {max_ep_len}번 했는데도 실패')\n",
    "#                     env.canvas()\n",
    "                \n",
    "                \n",
    "                \n",
    "            print_running_reward += current_ep_reward\n",
    "            print_running_episodes += 1\n",
    "\n",
    "            i_episode += 1\n",
    "        if epi+1 == le:\n",
    "            print(f'총 시행 수 : {le}\\n성공수 : {cnt}\\n성공률 : {(cnt/le)*100}%')\n",
    "            print(f'평균 행동 수 : {(pe*max_ep_len+to)/le}')\n",
    "            print(\"time :\", time.time() - start)\n",
    "            break\n",
    "            \n",
    "\n",
    "    # print total training time\n",
    "    print(\"============================================================================================\")\n",
    "    end_time = datetime.now().replace(microsecond=0)\n",
    "    print(\"Started training at (GMT) : \", start_time)\n",
    "    print(\"Finished training at (GMT) : \", end_time)\n",
    "    print(\"Total training time  : \", end_time - start_time)\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d83a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
