{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe7344c",
   "metadata": {},
   "source": [
    "# 1. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02b2de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -0.1 -0.3 5\n"
     ]
    }
   ],
   "source": [
    "from string import ascii_uppercase\n",
    "from draw_utils import *\n",
    "from pyglet.gl import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# reward\n",
    "move_reward = -0.1\n",
    "obs_reward = -0.3\n",
    "goal_reward = 5\n",
    "print('reward:' , move_reward, obs_reward, goal_reward)\n",
    "\n",
    "local_path = '/home/zlxlekta924/YC' #os.path.abspath(os.path.join(os.path.dirname(__file__)))\n",
    "\n",
    "\n",
    "class Simulator:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        height : 그리드 높이\n",
    "        width : 그리드 너비 \n",
    "        inds : A ~ Q alphabet list\n",
    "        '''\n",
    "        # Load train data\n",
    "        self.files = pd.read_csv(os.path.join(local_path, \"./data/factory_order_train.csv\"))\n",
    "        self.height = 10\n",
    "        self.width = 9\n",
    "        self.inds = list(ascii_uppercase)[:17]\n",
    "        self.clear_item = False\n",
    "\n",
    "    def set_box(self):\n",
    "        '''\n",
    "        아이템들이 있을 위치를 미리 정해놓고 그 위치 좌표들에 아이템이 들어올 수 있으므로 그리드에 100으로 표시한다.\n",
    "        데이터 파일에서 이번 에피소드 아이템 정보를 받아 가져와야 할 아이템이 있는 좌표만 -100으로 표시한다.\n",
    "        self.local_target에 에이전트가 이번에 방문해야할 좌표들을 저장한다.\n",
    "        따라서 가져와야하는 아이템 좌표와 end point 좌표(처음 시작했던 좌표로 돌아와야하므로)가 들어가게 된다.\n",
    "        '''\n",
    "        box_data = pd.read_csv(os.path.join(local_path, \"./data/box.csv\"))\n",
    "\n",
    "        # 물건이 들어있을 수 있는 경우\n",
    "        for box in box_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(box, \"row\")][getattr(box, \"col\")] = 0     #####################수정 100 -> 0\n",
    "\n",
    "        # 물건이 실제 들어있는 경우\n",
    "        order_item = list(set(self.inds) & set(self.items))\n",
    "        order_csv = box_data[box_data['item'].isin(order_item)]\n",
    "    \n",
    "        #print(order_csv) ######################################## 수정\n",
    "        \n",
    "        for order_box in order_csv.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(order_box, \"row\")][getattr(order_box, \"col\")] = -100   ###################### 수정 -100 -> 0\n",
    "            # local target에 가야 할 위치 좌표 넣기\n",
    "            self.local_target.append(\n",
    "                [getattr(order_box, \"row\"),\n",
    "                 getattr(order_box, \"col\")]\n",
    "                )\n",
    "        self.local_target.append([9,4]) \n",
    "#         self.grid[self.local_target[0][0]][self.local_target[0][1]] = -100 #############################수정\n",
    "        # 알파벳을 Grid에 넣어서 -> grid에 2Dconv 적용 가능\n",
    "\n",
    "    def set_obstacle(self):\n",
    "        '''\n",
    "        장애물이 있어야하는 위치는 미리 obstacles.csv에 정의되어 있다. 이 좌표들을 0으로 표시한다.\n",
    "        '''\n",
    "        obstacles_data = pd.read_csv(os.path.join(local_path, \"./data/obstacles.csv\"))\n",
    "        for obstacle in obstacles_data.itertuples(index = True, name ='Pandas'):\n",
    "            self.grid[getattr(obstacle, \"row\")][getattr(obstacle, \"col\")] = 0\n",
    "\n",
    "    def reset(self, epi):\n",
    "        '''\n",
    "        reset()은 첫 스텝에서 사용되며 그리드에서 에이전트 위치가 start point에 있게 한다.\n",
    "\n",
    "        :param epi: episode, 에피소드 마다 가져와야 할 아이템 리스트를 불러올 때 사용\n",
    "        :return: 초기셋팅 된 그리드\n",
    "        :rtype: numpy.ndarray\n",
    "        _____________________________________________________________________________________\n",
    "        items : 이번 에피소드에서 가져와야하는 아이템들\n",
    "        terminal_location : 현재 에이전트가 찾아가야하는 목적지\n",
    "        local_target : 한 에피소드에서 찾아가야하는 아이템 좌표, 마지막 엔드 포인트 등의 위치좌표들\n",
    "        actions: visualization을 위해 에이전트 action을 저장하는 리스트\n",
    "        curloc : 현재 위치\n",
    "        '''\n",
    "\n",
    "        # initial episode parameter setting\n",
    "        self.epi = epi\n",
    "        self.items = list(self.files.iloc[self.epi])[0]\n",
    "        self.cumulative_reward = 0\n",
    "        self.terminal_location = None\n",
    "        self.local_target = []\n",
    "        self.actions = []\n",
    "\n",
    "        # initial grid setting\n",
    "        self.grid = np.ones((self.height, self.width), dtype=\"float16\")\n",
    "\n",
    "        # set information about the gridworld\n",
    "        self.set_box()\n",
    "        self.set_obstacle()\n",
    "\n",
    "        # start point를 grid에 표시\n",
    "        self.curloc = [9, 4]\n",
    "        self.grid[int(self.curloc[0])][int(self.curloc[1])] = -5\n",
    "\n",
    "\n",
    "        self.done = False\n",
    "        \n",
    "        #print('###########################')  ###################################수정한 부분\n",
    "        #print(f'items loc : {self.local_target}')\n",
    "        \n",
    "        \n",
    "        return self.grid\n",
    "\n",
    "    def apply_action(self, action, cur_x, cur_y):\n",
    "        '''\n",
    "        에이전트가 행한 action대로 현 에이전트의 위치좌표를 바꾼다.\n",
    "        action은 discrete하며 4가지 up,down,left,right으로 정의된다.\n",
    "        \n",
    "        :param x: 에이전트의 현재 x 좌표\n",
    "        :param y: 에이전트의 현재 y 좌표\n",
    "        :return: action에 따라 변한 에이전트의 x 좌표, y 좌표\n",
    "        :rtype: int, int\n",
    "        '''\n",
    "        new_x = cur_x\n",
    "        new_y = cur_y\n",
    "        # up\n",
    "        if action == 0:\n",
    "            new_x = cur_x - 1\n",
    "        # down\n",
    "        elif action == 1:\n",
    "            new_x = cur_x + 1\n",
    "        # left\n",
    "        elif action == 2:\n",
    "            new_y = cur_y - 1\n",
    "        # right\n",
    "        else:\n",
    "            new_y = cur_y + 1\n",
    "\n",
    "        return int(new_x), int(new_y)\n",
    "\n",
    "\n",
    "    def get_reward(self, new_x, new_y, out_of_boundary):\n",
    "        '''\n",
    "        get_reward함수는 리워드를 계산하는 함수이며, 상황에 따라 에이전트가 action을 옳게 했는지 판단하는 지표가 된다.\n",
    "\n",
    "        :param new_x: action에 따른 에이전트 새로운 위치좌표 x\n",
    "        :param new_y: action에 따른 에이전트 새로운 위치좌표 y\n",
    "        :param out_of_boundary: 에이전트 위치가 그리드 밖이 되지 않도록 제한\n",
    "        :return: action에 따른 리워드\n",
    "        :rtype: float\n",
    "        '''\n",
    "\n",
    "        # 바깥으로 나가는 경우\n",
    "        if any(out_of_boundary):\n",
    "            reward = obs_reward\n",
    "                       \n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 \n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                reward = obs_reward  \n",
    "\n",
    "            # 현재 목표에 도달한 경우\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "                reward = goal_reward\n",
    "\n",
    "            # 그냥 움직이는 경우 \n",
    "            else:\n",
    "                reward = move_reward\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        ''' \n",
    "        에이전트의 action에 따라 step을 진행한다.\n",
    "        action에 따라 에이전트 위치를 변환하고, action에 대해 리워드를 받고, 어느 상황에 에피소드가 종료되어야 하는지 등을 판단한다.\n",
    "        에이전트가 endpoint에 도착하면 gif로 에피소드에서 에이전트의 행동이 저장된다.\n",
    "\n",
    "        :param action: 에이전트 행동\n",
    "        :return:\n",
    "            grid, 그리드\n",
    "            reward, 리워드\n",
    "            cumulative_reward, 누적 리워드\n",
    "            done, 종료 여부\n",
    "            goal_ob_reward, goal까지 아이템을 모두 가지고 돌아오는 finish율 계산을 위한 파라미터\n",
    "\n",
    "        :rtype: numpy.ndarray, float, float, bool, bool/str\n",
    "\n",
    "        (Hint : 시작 위치 (9,4)에서 up말고 다른 action은 전부 장애물이므로 action을 고정하는 것이 좋음)\n",
    "        '''\n",
    "        if self.local_target:\n",
    "            self.terminal_location = self.local_target[0]\n",
    "            \n",
    "        cur_x,cur_y = self.curloc\n",
    "        self.actions.append((cur_x, cur_y))\n",
    "\n",
    "        goal_ob_reward = False\n",
    "        \n",
    "        new_x, new_y = self.apply_action(action, cur_x, cur_y)\n",
    "\n",
    "        out_of_boundary = [new_x < 0, new_x >= self.height, new_y < 0, new_y >= self.width]\n",
    "\n",
    "        # 바깥으로 나가는 경우 종료\n",
    "        if any(out_of_boundary):\n",
    "            reward = self.get_reward(new_x, new_y, out_of_boundary) ######################수정\n",
    "            # self.done = True\n",
    "            # goal_ob_reward = True\n",
    "            \n",
    "        else:\n",
    "            # 장애물에 부딪히는 경우 종료\n",
    "            if self.grid[new_x][new_y] == 0:\n",
    "                # self.done = True\n",
    "                # goal_ob_reward = True\n",
    "                reward = self.get_reward(new_x, new_y, out_of_boundary) ###############################수정\n",
    "\n",
    "            # 현재 목표에 도달한 경우, 다음 목표설정\n",
    "            elif new_x == self.terminal_location[0] and new_y == self.terminal_location[1]:\n",
    "\n",
    "\n",
    "                # end point 일 때\n",
    "                if [new_x, new_y] == [9,4]:\n",
    "                    self.done = True\n",
    "                \n",
    "                self.local_target.remove(self.local_target[0])\n",
    "                \n",
    "                self.grid[cur_x][cur_y] = 1\n",
    "                self.grid[new_x][new_y] = -5\n",
    "                goal_ob_reward = True\n",
    "                self.curloc = [new_x, new_y]\n",
    "                \n",
    "                reward = self.get_reward(new_x, new_y, out_of_boundary)\n",
    "            else:\n",
    "                # 그냥 움직이는 경우 \n",
    "                \n",
    "                self.grid[cur_x][cur_y] = 1    \n",
    "                self.grid[new_x][new_y] = -5\n",
    "                    \n",
    "                \n",
    "                self.curloc = [new_x,new_y]\n",
    "                \n",
    "                reward = self.get_reward(new_x, new_y, out_of_boundary)\n",
    "                \n",
    "        #reward = self.get_reward(new_x, new_y, out_of_boundary)  ################################# 수정\n",
    "        #print('reward : ' ,reward) ####################################수정한 부분\n",
    "        \n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        if self.done == True:\n",
    "            if [new_x, new_y] == [9, 4]:\n",
    "                if self.terminal_location == [9, 4]:\n",
    "                    # 완료되면 GIFS 저장\n",
    "                    goal_ob_reward = 'finish'\n",
    "                    height = 10\n",
    "                    width = 9 \n",
    "                    display = Display(visible=False, size=(width, height))\n",
    "                    display.start()\n",
    "\n",
    "                    start_point = (9, 4)\n",
    "                    unit = 50\n",
    "                    screen_height = height * unit\n",
    "                    screen_width = width * unit\n",
    "                    log_path = \"./logs\"\n",
    "                    data_path = \"./data\"\n",
    "                    render_cls = Render(screen_width, screen_height, unit, start_point, data_path, log_path)\n",
    "                    for idx, new_pos in enumerate(self.actions):\n",
    "                        render_cls.update_movement(new_pos, idx+1)\n",
    "                    \n",
    "                    render_cls.save_gif(self.epi)\n",
    "                    render_cls.viewer.close()\n",
    "                    display.stop()\n",
    "        \n",
    "        \n",
    "        return self.grid, reward, self.cumulative_reward, self.done, goal_ob_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3245a",
   "metadata": {},
   "source": [
    "## 1. Agent 구성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df27b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zlxlekta924/anaconda3/envs/test/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "n_rollout     = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "810439cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=1),\n",
    "#             nn.ReLU())\n",
    "        \n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "#             nn.ReLU())\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(90,128)\n",
    "        self.fc2 = nn.Linear(128,32)\n",
    "        self.fc_pi = nn.Linear(32,4)\n",
    "        self.fc_v = nn.Linear(32,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = torch.reshape(x, (-1, 1, 10, 9)).to('cuda')\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim) #, dim=softmax_dim\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = torch.reshape(x, (-1, 1, 10, 9)).to('cuda')\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "    \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s,a,r,s_prime,done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_lst.append([done_mask])\n",
    "        \n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                                               torch.tensor(done_lst, dtype=torch.float)\n",
    "        self.data = []\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "  \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done = self.make_batch()\n",
    "        td_target = torch.tensor(r,device='cuda') + torch.tensor(gamma, device='cuda') * self.v(s_prime) * torch.tensor(done, device='cuda')\n",
    "        delta = td_target - self.v(s)\n",
    "        pi = self.pi(s, softmax_dim=1)\n",
    "        pi_a = pi.gather(1,a.to('cuda'))\n",
    "        loss = -(torch.log(pi_a)+1e9) * (delta.detach()+1e9) + F.smooth_l1_loss(self.v(s), td_target.detach())\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward()\n",
    "        self.optimizer.step() \n",
    "        torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b33bac",
   "metadata": {},
   "source": [
    "#### 전체적인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbb79aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/1 [00:00<?, ?it/s]\n",
      "  0%|                                                                                 | 0/2000 [00:00<?, ?it/s]\u001b[A/home/zlxlekta924/anaconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "  0%|                                                                       | 1/2000 [00:02<1:09:30,  2.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :0, avg score : -1.221\n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 0.  1.  1. -5.  1.  1.  1.  1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 0.  1.  0.  1.  0.  1.  0.  1.  0.]\n",
      " [ 1.  1.  0.  1.  0.  1.  0.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                       | 2/2000 [00:03<1:05:06,  1.96s/it]\u001b[A\n",
      "  0%|                                                                       | 3/2000 [00:05<1:04:24,  1.94s/it]\u001b[A\n",
      "  0%|▏                                                                      | 4/2000 [00:07<1:03:55,  1.92s/it]\u001b[A\n",
      "  0%|▏                                                                      | 5/2000 [00:10<1:07:56,  2.04s/it]\u001b[A\n",
      "  0%|▏                                                                      | 6/2000 [00:12<1:09:20,  2.09s/it]\u001b[A\n",
      "  0%|▏                                                                      | 7/2000 [00:14<1:10:49,  2.13s/it]\u001b[A\n",
      "  0%|▎                                                                      | 8/2000 [00:16<1:07:30,  2.03s/it]\u001b[A\n",
      "  0%|▎                                                                      | 9/2000 [00:18<1:08:38,  2.07s/it]\u001b[A\n",
      "  0%|▎                                                                     | 10/2000 [00:20<1:09:05,  2.08s/it]\u001b[A\n",
      "  1%|▍                                                                     | 11/2000 [00:22<1:05:44,  1.98s/it]\u001b[A\n",
      "  1%|▍                                                                     | 12/2000 [00:24<1:03:43,  1.92s/it]\u001b[A\n",
      "  1%|▍                                                                     | 13/2000 [00:26<1:05:36,  1.98s/it]\u001b[A\n",
      "  1%|▍                                                                     | 14/2000 [00:28<1:04:11,  1.94s/it]\u001b[A\n",
      "  1%|▌                                                                     | 15/2000 [00:29<1:02:36,  1.89s/it]\u001b[A\n",
      "  1%|▌                                                                     | 16/2000 [00:31<1:01:27,  1.86s/it]\u001b[A\n",
      "  1%|▌                                                                     | 17/2000 [00:33<1:00:36,  1.83s/it]\u001b[A\n",
      "  1%|▋                                                                     | 18/2000 [00:35<1:00:03,  1.82s/it]\u001b[A\n",
      "  1%|▋                                                                     | 19/2000 [00:36<1:00:08,  1.82s/it]\u001b[A\n",
      "  1%|▋                                                                     | 20/2000 [00:38<1:00:53,  1.84s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "def main():  \n",
    "    env = Simulator()\n",
    "    \n",
    "    model = ActorCritic()\n",
    "#     model.load_state_dict(torch.load('model_weights.pth'))\n",
    "#     model.eval()\n",
    "    \n",
    "    model = model.to('cuda')\n",
    "    files = pd.read_csv(\"./data/factory_order_train.csv\")\n",
    "    print_interval = 50\n",
    "    for epi in tqdm(range(1)):\n",
    "#         time.sleep(0.1)\n",
    "#         done = False\n",
    "#         s = env.reset(epi)\n",
    "#         items = list(files.iloc[epi%40000])[0]\n",
    "#         score = 0.0\n",
    "#         s = np.asarray(s, dtype=np.float32)\n",
    "#         a_step= 0\n",
    "#         first = True\n",
    "#         end = False\n",
    "#         if epi % 100 == 0:\n",
    "#             torch.save(model.state_dict(), '/content/drive/MyDrive/Study/RL/weights/model_weights.pth')\n",
    "\n",
    "        #for i in range(50):\n",
    "        for i in tqdm(range(2000)):\n",
    "            done = False\n",
    "            s = env.reset(epi)\n",
    "            items = list(files.iloc[epi%40000])[0]\n",
    "            score = 0.0\n",
    "            s = np.asarray(s, dtype=np.float32)\n",
    "            a_step= 0\n",
    "            first = True\n",
    "            end = False\n",
    "            while not done:\n",
    "                for t in range(n_rollout): # n_rollout을 통해 for loop 진행 후 학습 진행\n",
    "                    # 어디로 갈지 확률 예측 부분\n",
    "                    prob = model.pi(torch.from_numpy(s).float())\n",
    "                    m = Categorical(prob)\n",
    "                    a = m.sample().item()\n",
    "\n",
    "                    if first:\n",
    "                        a = 0\n",
    "                        first = False\n",
    "                        \n",
    "\n",
    "                    s_prime, r, cumul, done, goal_reward = env.step(a)\n",
    "                    \n",
    "                    # 스텝 종료 패널티\n",
    "                    if r < 0 and r !=-5:\n",
    "                        r = r + r*(a_step//30)*0.1\n",
    "                    a_step +=1\n",
    "                    if a_step == 300:\n",
    "                        end = True\n",
    "                        break\n",
    "                    \n",
    "                    # 스텝이 늘어날 수록 강해지는 패널티\n",
    "                    \n",
    "                    view = s_prime\n",
    "                    \n",
    "                    s_prime = np.asarray(s_prime, dtype=np.float32)\n",
    "                    \n",
    "                    model.put_data((s,a,r,s_prime,done))\n",
    "                    \n",
    "                    s = s_prime\n",
    "                    score += r\n",
    "\n",
    "                    if done==True or end==True:\n",
    "                        break\n",
    "\n",
    "                if done == True or end ==True:\n",
    "                    break\n",
    "                model.train_net() # 학습 진행 코드\n",
    "\n",
    "            if i%print_interval==0 :\n",
    "                print(\"# of episode :{}, avg score : {:.3f}\".format(i, score/print_interval))\n",
    "                print(view)\n",
    "                score = 0.0\n",
    "                \n",
    "        if epi%print_interval==0 and epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.3f}\".format(epi, score/print_interval))\n",
    "            print(view)\n",
    "            score = 0.0\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1116dfad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
