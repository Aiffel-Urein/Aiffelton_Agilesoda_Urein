# 👽 각 알고리즘 별 중간점검 

1. DQN
2. AC
3. PPO

## 1. DQN model

epi 1으로 5시간 학습시킨 뒤 모든 train 데이터 1회씩 학습

### 환경
- 아이템 창고 100 아이템 200 에이전트 50 길 1 장애물 150
- 모든 아이템 및 아이템 창고 표시 및 순서 고려 x
- 먹은 아이템 위치는 길 1 로 변경

### 보상
move : -1  
obstacle : -10  
item : 1000  
no_item : -1.5  
all item : 2000  

### 에이전트

- a_step : 100
- Con2D 2계층, FC layer 2계층


<img src='./dqn.gif'>

### 성공률 0%

- 특정 epi 에 최적화 시킨 뒤 다양한 epi 를 학습시키는 순서가 합리적인지 토의 필요
- a_step 을 증가할 필요
- 보상 다양화는 유의미하다고 판단
- no_item 공간에 들어갈 수 있는 대신 음수의 리워드를 주는 방식 타당하다고 판단.

---


## 2. AC model

약 5시간 학습 : train_data 약 50000번 학습

### 환경

- 아이템 2 에이전트 3 길 1 장애물 0 으로 표시
- 모든 아이템이 표시 및 순서 고려 x
- 먹은 아이템 위치는 장애물 0 로 변경

### 보상
move :-0.1  
obstacle : -0.2  
item : 1000  

### 에이전트

- a_step : 200
- FC layer 3계층으로 구성

<img src='./ac1.gif'>
<img src='./ac2.gif'>

### 성공률 2/1226

act = 161, 174.
성공한 두 test 모두 아이템이 7개인 공통점 있음

- DQN 에서의 좌표 데이터를 가져와서 실험할 필요성 있음
- DQN 에서의 CNN 구조로 실험할 필요성 있음
- 현재 가장 나은 결과

---

## 3. PPO

3시간 학습

### 환경
- AC 와 동일

### 보상

- AC 와 동일

### 에이전트
- a_step : 300
- Con2D 2계층, FC layer 2계층 (AC 와 동일)

<img src='./ppo.gif'>

### 성공률 0%

- 코드 단순화 및 장시간 학습 필요


```python

```
